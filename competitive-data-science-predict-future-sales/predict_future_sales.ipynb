{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "from fastai.imports import *\n",
    "from fastai.transforms import *\n",
    "from fastai.conv_learner import *\n",
    "from fastai.model import *\n",
    "from fastai.dataset import *\n",
    "from fastai.sgdr import *\n",
    "from fastai.plots import *\n",
    "from fastai.dataset import *\n",
    "from fastai.structured import *\n",
    "from fastai.column_data import *\n",
    "\n",
    "from math import ceil\n",
    "\n",
    "from dateutil import parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.set_device(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fastai; fastai.__file__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = \"data/\"\n",
    "sz=224\n",
    "arch=resnext101_64\n",
    "batch_size=5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploring Other Options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-069d250d7f16>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mitem_categories\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'{PATH}item_categories.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mitems\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'{PATH}items.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mshops\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'{PATH}shops.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mtrain\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'{PATH}sales_train.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mtest\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'{PATH}test.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "item_categories = pd.read_csv(f'{PATH}item_categories.csv')\n",
    "items = pd.read_csv(f'{PATH}items.csv')\n",
    "shops = pd.read_csv(f'{PATH}shops.csv')\n",
    "train = pd.read_csv(f'{PATH}sales_train.csv')\n",
    "test = pd.read_csv(f'{PATH}test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import product\n",
    "# Create \"grid\" with columns\n",
    "index_cols = ['shop_id', 'item_id', 'date_block_num']\n",
    "\n",
    "# For every month we create a grid from all shops/items combinations from that month\n",
    "grid = []\n",
    "for block_num in train['date_block_num'].unique():\n",
    "    cur_shops = train.loc[train['date_block_num'] == block_num, 'shop_id'].unique()\n",
    "    cur_items = train.loc[train['date_block_num'] == block_num, 'item_id'].unique()\n",
    "    grid.append(np.array(list(product(*[cur_shops, cur_items, [block_num]])),dtype='int32'))\n",
    "grid = pd.DataFrame(np.vstack(grid), columns = index_cols,dtype=np.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train[train.item_price<100000]\n",
    "train = train[train.item_cnt_day<=1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train.merge(items)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train.merge(item_categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales_m = sales_m.merge(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales_m = train.groupby(['date_block_num','shop_id','item_id']).agg({'item_cnt_day': 'sum','item_price': np.mean}).reset_index()\n",
    "sales_m = pd.merge(grid,sales_m,on=['date_block_num','shop_id','item_id'],how='left').fillna(0)\n",
    "# adding the category id too\n",
    "sales_m = pd.merge(sales_m,items,on=['item_id'],how='left')\n",
    "\n",
    "for type_id in ['item_id','shop_id','item_category_id']:\n",
    "    for column_id,aggregator,aggtype in [('item_price',np.mean,'avg'),('item_cnt_day',np.sum,'sum'),('item_cnt_day',np.mean,'avg')]:\n",
    "\n",
    "        mean_df = train.groupby([type_id,'date_block_num']).aggregate(aggregator).reset_index()[[column_id,type_id,'date_block_num']]\n",
    "        mean_df.columns = [type_id+'_'+aggtype+'_'+column_id,type_id,'date_block_num']\n",
    "\n",
    "        sales_m = pd.merge(sales_m,mean_df,on=['date_block_num',type_id],how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales_m = train.groupby(['date_block_num','shop_id','item_id']).agg({'item_cnt_day': 'sum','item_price': np.mean}).reset_index()\n",
    "sales_m = pd.merge(grid,train,on=['date_block_num','shop_id','item_id'],how='left').fillna(0)\n",
    "# adding the category id too\n",
    "sales_m = pd.merge(sales_m,items,on=['item_id'],how='left')\n",
    "\n",
    "for type_id in ['item_id','shop_id','item_category_id']:\n",
    "    for column_id,aggregator,aggtype in [('item_price',np.mean,'avg'),('item_cnt_day',np.sum,'sum'),('item_cnt_day',np.mean,'avg')]:\n",
    "\n",
    "        mean_df = train.groupby([type_id,'date_block_num']).aggregate(aggregator).reset_index()[[column_id,type_id,'date_block_num']]\n",
    "        mean_df.columns = [type_id+'_'+aggtype+'_'+column_id,type_id,'date_block_num']\n",
    "\n",
    "        sales_m = pd.merge(sales_m,mean_df,on=['date_block_num',type_id],how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lag_variables  = list(sales_m.columns[7:])+['item_cnt_day']\n",
    "lags = [1 ,2 ,3 ,4, 5, 12]\n",
    "for lag in lags:\n",
    "    sales_new_df = sales_m.copy()\n",
    "    sales_new_df.date_block_num+=lag\n",
    "    sales_new_df = sales_new_df[['date_block_num','shop_id','item_id']+lag_variables]\n",
    "    sales_new_df.columns = ['date_block_num','shop_id','item_id']+ [lag_feat+'_lag_'+str(lag) for lag_feat in lag_variables]\n",
    "    sales_means = pd.merge(sales_means, sales_new_df,on=['date_block_num','shop_id','item_id'] ,how='left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploratory Analysis and Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls {PATH}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_subm = pd.read_csv(f'{PATH}sample_submission.csv')\n",
    "display(sample_subm.tail())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "item_categories = pd.read_csv(f'{PATH}item_categories.csv')\n",
    "items = pd.read_csv(f'{PATH}items.csv')\n",
    "shops = pd.read_csv(f'{PATH}shops.csv')\n",
    "train = pd.read_csv(f'{PATH}sales_train.csv')\n",
    "test = pd.read_csv(f'{PATH}test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we'll be dropping this shortly, but it will be readded later \n",
    "temp_item_price = train['item_price']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Make Monthly\n",
    "train['date'] = pd.to_datetime(train['date'], format='%d.%m.%Y')\n",
    "train['month'] = train['date'].dt.month\n",
    "train['year'] = train['date'].dt.year\n",
    "train = train.drop(['date','item_price'], axis=1)\n",
    "train = train.groupby([c for c in train.columns if c not in ['item_cnt_day']], as_index=False)[['item_cnt_day']].sum()\n",
    "train = train.rename(columns={'item_cnt_day':'item_cnt_month'})\n",
    "#Monthly Mean\n",
    "shop_item_monthly_mean = train[['shop_id','item_id','item_cnt_month']].groupby(['shop_id','item_id'], as_index=False)[['item_cnt_month']].mean()\n",
    "shop_item_monthly_mean = shop_item_monthly_mean.rename(columns={'item_cnt_month':'item_cnt_month_mean'})\n",
    "#Add Mean Feature\n",
    "train = pd.merge(train, shop_item_monthly_mean, how='left', on=['shop_id','item_id'])\n",
    "#Last Month (Oct 2015)\n",
    "shop_item_prev_month = train[train['date_block_num']==33][['shop_id','item_id','item_cnt_month']]\n",
    "shop_item_prev_month = shop_item_prev_month.rename(columns={'item_cnt_month':'item_cnt_prev_month'})\n",
    "shop_item_prev_month.head()\n",
    "#Add Previous Month Feature\n",
    "train = pd.merge(train, shop_item_prev_month, how='left', on=['shop_id','item_id']).fillna(0.)\n",
    "#Items features\n",
    "train = pd.merge(train, items, how='left', on='item_id')\n",
    "#Item Category features\n",
    "train = pd.merge(train, item_categories, how='left', on='item_category_id')\n",
    "#Shops features\n",
    "train = pd.merge(train, shops, how='left', on='shop_id')\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = test.drop('ID', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test['month'] = 11\n",
    "test['year'] = 2015\n",
    "test['date_block_num'] = 34\n",
    "#Add Mean Feature\n",
    "test = pd.merge(test, shop_item_monthly_mean, how='left', on=['shop_id','item_id']).fillna(0.)\n",
    "#Add Previous Month Feature\n",
    "test = pd.merge(test, shop_item_prev_month, how='left', on=['shop_id','item_id']).fillna(0.)\n",
    "#Items features\n",
    "test = pd.merge(test, items, how='left', on='item_id')\n",
    "#Item Category features\n",
    "test = pd.merge(test, item_categories, how='left', on='item_category_id')\n",
    "#Shops features\n",
    "test = pd.merge(test, shops, how='left', on='shop_id')\n",
    "test['item_cnt_month'] = 0.\n",
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['item_price'] = temp_item_price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "price = train.reset_index().set_index(\n",
    "    ['item_id', 'shop_id', 'date_block_num']).sort_index()[['item_price', 'item_cnt_month']]\n",
    "price.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helpers\n",
    "def closest_date_block(current_day, item_id, shop_id):\n",
    "    \"\"\"Find the block_date which is closest to the current_day, given item_id and shop_id. Returns index integer\"\"\"\n",
    "    if (item_id, shop_id) in price.index:\n",
    "        search_lst = np.array(price.loc[(item_id, shop_id)].index)        \n",
    "        return search_lst[np.abs(current_day - search_lst).argmin()]\n",
    "    return -1\n",
    "                \n",
    "def closest_price(current_day, item_id, shop_id):\n",
    "    closest_date = closest_date_block(current_day, item_id, shop_id)\n",
    "    if closest_date != -1:\n",
    "        return price.loc[( item_id, shop_id, closest_date )]['item_price']\n",
    "    return np.nan\n",
    "\n",
    "counter = 0\n",
    "from IPython.display import clear_output\n",
    "def closest_price_lambda(x):\n",
    "    global counter\n",
    "    if counter % 5000 == 0:\n",
    "        # hack to display progress, obviously this isn't a\n",
    "        # robust way to do things :)\n",
    "        clear_output()\n",
    "        print(\"{}/{}\".format(counter, len(test)))\n",
    "    counter += 1\n",
    "    result = closest_price(34, x.item_id, x.shop_id)\n",
    "    if counter == len(test):\n",
    "        clear_output()\n",
    "        print(\"DONE\")\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "closest_prices = test.apply(closest_price_lambda, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test['item_price'] = closest_prices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Applying Categorical Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_vars = [\n",
    "    'date_block_num',\n",
    "    'item_category_id',\n",
    "    'item_category_name',\n",
    "    'item_id',\n",
    "    'item_name',\n",
    "    'month',              \n",
    "    'shop_id',\n",
    "    'shop_name',\n",
    "    'year',               \n",
    "]\n",
    "contin_vars = [\n",
    "    'item_cnt_month_mean',\n",
    "    'item_cnt_prev_month',\n",
    "    'item_cnt_month',\n",
    "    'item_price',\n",
    "]\n",
    "dep = 'item_cnt_month'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "date_block_num_cat = pd.api.types.CategoricalDtype(categories=[i for i in range(35)], ordered=True)\n",
    "item_category_id_cat = pd.api.types.CategoricalDtype(categories=np.unique(item_categories['item_category_id']), ordered=True)\n",
    "item_category_name_cat = pd.api.types.CategoricalDtype(categories=np.unique(item_categories['item_category_name']), ordered=True)\n",
    "item_id_cat = pd.api.types.CategoricalDtype(categories=np.unique(items['item_id']), ordered=True)\n",
    "item_name_cat = pd.api.types.CategoricalDtype(categories=np.unique(items['item_name']), ordered=True)\n",
    "month_cat = pd.api.types.CategoricalDtype(categories=np.unique(train['month']), ordered=True)\n",
    "shop_id_cat = pd.api.types.CategoricalDtype(categories=np.unique(shops['shop_id']), ordered=True)\n",
    "shop_name_cat = pd.api.types.CategoricalDtype(categories=np.unique(shops['shop_name']), ordered=True)\n",
    "year_cat = pd.api.types.CategoricalDtype(categories=np.unique(train['year']), ordered=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['date_block_num'] = train['date_block_num'].astype(date_block_num_cat)\n",
    "train['item_id'] = train['item_id'].astype(item_id_cat)\n",
    "train['item_category_id'] = train['item_category_id'].astype(item_category_id_cat)\n",
    "train['item_category_name'] = train['item_category_name'].astype(item_category_name_cat)\n",
    "train['item_name'] = train['item_name'].astype(item_name_cat)\n",
    "train['shop_id'] = train['shop_id'].astype(shop_id_cat)\n",
    "train['shop_name'] = train['shop_name'].astype(shop_name_cat)\n",
    "train['month'] = train['month'].astype(month_cat)\n",
    "train['year'] = train['year'].astype(year_cat)\n",
    "\n",
    "test['date_block_num'] = test['date_block_num'].astype(date_block_num_cat)\n",
    "test['item_id'] = test['item_id'].astype(item_id_cat)\n",
    "test['item_category_id'] = test['item_category_id'].astype(item_category_id_cat)\n",
    "test['item_category_name'] = test['item_category_name'].astype(item_category_name_cat)\n",
    "test['item_name'] = test['item_name'].astype(item_name_cat)\n",
    "test['shop_id'] = test['shop_id'].astype(shop_id_cat)\n",
    "test['shop_name'] = test['shop_name'].astype(shop_name_cat)\n",
    "test['month'] = test['month'].astype(month_cat)\n",
    "test['year'] = test['year'].astype(year_cat)\n",
    "\n",
    "for v in contin_vars:\n",
    "    train[v] = train[v].astype('float32')\n",
    "    test[v] = test[v].astype('float32')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert not train.isnull().any().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert not test.drop('item_price', axis=1).isnull().any().any() # item price can be nan by design"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.to_pickle(PATH + 'train.pkl')\n",
    "test.to_pickle(PATH + 'test.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls {PATH}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## _Deep Learning Approach_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_pickle(PATH + 'train.pkl')\n",
    "test = pd.read_pickle(PATH + 'test.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rmse(y_pred, targ):\n",
    "    return math.sqrt(((targ - y_pred)**2).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val = train[train['date_block_num']==33]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train[train['date_block_num']<33]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df, y, nas, mapper = proc_df(train, dep, do_scale=True)\n",
    "val_df, val_y, nas, mapper = proc_df(\n",
    "    val,\n",
    "    dep,\n",
    "    do_scale=True,\n",
    "    mapper=mapper,\n",
    ")\n",
    "test_df, _, nas, mapper = proc_df(\n",
    "    test,\n",
    "    dep,\n",
    "    do_scale=True,\n",
    "    mapper=mapper,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = test_df.drop('item_price_na', axis=1) # ignore these"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = np.log1p(y.clip(0.,40.))\n",
    "y_val = np.log1p(val_y.clip(0.,40.))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "md = ColumnarModelData.from_data_frames(\n",
    "    path=PATH,\n",
    "    trn_df=df,\n",
    "    trn_y=y_train,\n",
    "    val_df=val_df,\n",
    "    val_y=y_val,\n",
    "    cat_flds=cat_vars,\n",
    "    bs=256,\n",
    "    test_df=test_df,\n",
    "    is_reg=True,  # is regression\n",
    "    is_multi=False, \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_sz = [(c, len(train[c].cat.categories)+1) for c in cat_vars]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_sz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_szs = [(c, min(50, (c+1)//2)) for _,c in cat_sz]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_szs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = md.get_learner(\n",
    "    emb_szs=emb_szs,\n",
    "    n_cont=len(df.columns) - len(cat_vars),\n",
    "    emb_drop=.05,\n",
    "    out_sz=1,\n",
    "    szs=[1000, 500],\n",
    "    drops=[.001, .01],\n",
    "    use_bn=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m.lr_find(1e-7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m.sched.plot(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "lr = 1e-5\n",
    "m.fit(lrs=lr, n_cycle=3, metrics=[rmse], cycle_len=1, cycle_mult=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m.fit(1e-5, n_cycle=2, metrics=[rmse])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submit It!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = m.predict(is_test=True)\n",
    "predictions = np.expm1(result).clip(0, 20)\n",
    "display(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_subm.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_dataframe = pd.read_csv(f'{PATH}test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_dataframe['item_cnt_month'] = predictions\n",
    "final_dataframe = final_dataframe[['ID', 'item_cnt_month']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_dataframe.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(final_dataframe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(np.unique(final_dataframe['ID']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SUBM = 'sub/'\n",
    "os.makedirs(SUBM, exist_ok=True)\n",
    "final_dataframe.to_csv(f'{SUBM}subm.gz', compression='gzip', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FileLink(f'{SUBM}subm.gz')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
